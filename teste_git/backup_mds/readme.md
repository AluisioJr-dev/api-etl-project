# UOLCatLovers - Cat Facts Data Engineering Project

## üìã Sobre o Projeto

A **UOLCatLovers** √© uma startup de tecnologia pet que desenvolve um aplicativo m√≥vel fornecendo fatos interessantes sobre gatos para seus usu√°rios. Este reposit√≥rio cont√©m as solu√ß√µes de engenharia de dados para extra√ß√£o, armazenamento e disponibiliza√ß√£o dos dados de fatos sobre gatos.

Os dados s√£o extra√≠dos da **Cat Facts API**.  
üìñ Documenta√ß√£o: https://alexwohlbruck.github.io/cat-facts/docs/

---

## üéØ Objetivos do Projeto

Este projeto foi desenvolvido para atender √†s seguintes necessidades:

1. **Extra√ß√£o inicial de dados**: Script Python para extrair dados da API e salvar localmente em CSV
2. **Arquitetura em nuvem**: Solu√ß√£o escal√°vel na Google Cloud Platform (GCP)
3. **Esquema de dados**: Defini√ß√£o de tabela BigQuery para an√°lise
4. **Consultas SQL**: Queries para atender demandas dos times de Analytics e Desenvolvimento

---


## üìÇ Estrutura do Reposit√≥rio

```
teste_git/
‚îú‚îÄ‚îÄ readme.md                  # Este arquivo
‚îú‚îÄ‚îÄ CHANGELOG.md               # Hist√≥rico de altera√ß√µes
‚îú‚îÄ‚îÄ executar_v1_heroku.ps1     # Script PowerShell: executa pipeline v1 (Heroku/offline)
‚îú‚îÄ‚îÄ executar_v2_ninja.ps1      # Script PowerShell: executa pipeline v2 (catfact.ninja/online)
‚îú‚îÄ‚îÄ cat_facts_collector/       # Pipelines e documenta√ß√£o principal
‚îÇ   ‚îú‚îÄ‚îÄ README.md                  # Documenta√ß√£o geral dos pipelines
‚îÇ   ‚îú‚îÄ‚îÄ v1_cat_fact_official_2026_01/   # Pipeline v1 (API Heroku, atualmente offline)
‚îÇ   ‚îú‚îÄ‚îÄ v2_catfact_ninja_2026_01/       # Pipeline v2 (API catfact.ninja, funcional)
‚îÇ   ‚îî‚îÄ‚îÄ bigquery_schema/                # Modelos, queries e documenta√ß√£o do BigQuery
‚îî‚îÄ‚îÄ ...
```
---

## üöÄ Solu√ß√µes Desenvolvidas

### 1. Script Python para Extra√ß√£o de Dados (Solu√ß√£o Local)

**Objetivo**: Desenvolver um script Python que extraia dados da Cat Facts API e salve em arquivo CSV local.

**Localiza√ß√£o**: `src/extract_cat_facts.py`

**Funcionalidades**:
- Consumo da API Cat Facts
- Tratamento de erros e retry logic
- Salvamento em formato CSV
- Logging de execu√ß√£o

**Como executar**:
```bash
pip install -r requirements.txt
python src/extract_cat_facts.py
```

---

### 2. Arquitetura na Google Cloud Platform

**Objetivo**: Projetar arquitetura escal√°vel para extrair, armazenar e disponibilizar dados na nuvem.

**Localiza√ß√£o**: `architecture/gcp_architecture.md`

Este projeto apresenta **duas abordagens de orquestra√ß√£o** para atender diferentes necessidades:

---

#### üîπ **Arquitetura 1: Solu√ß√£o Serverless com Cloud Scheduler**

**Ideal para**: Pipelines simples, baixo volume de dados, execu√ß√µes agendadas regulares

**Componentes**:
- **Cloud Scheduler**: Agendamento de execu√ß√µes peri√≥dicas (cron jobs)
- **Cloud Functions**: Fun√ß√£o serverless para extra√ß√£o da API
- **Cloud Storage**: Armazenamento de dados brutos (Data Lake - camada Bronze)
- **Cloud Dataflow**: Processamento e transforma√ß√£o de dados (ETL)
- **BigQuery**: Data Warehouse para consultas anal√≠ticas
- **Cloud Logging/Monitoring**: Observabilidade, logs e alertas
- **Pub/Sub** (opcional): Mensageria para comunica√ß√£o ass√≠ncrona entre componentes

**Fluxo de dados**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cloud Scheduler ‚îÇ (Trigger di√°rio: 0 2 * * *)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cloud Function  ‚îÇ (Extra√ß√£o da API Cat Facts)
‚îÇ  extract_facts  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cloud Storage   ‚îÇ (gs://bucket-name/raw/cat_facts_{date}.json)
‚îÇ   (Raw/Bronze)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cloud Dataflow ‚îÇ (Transforma√ß√£o e Valida√ß√£o)
‚îÇ   (ETL Pipeline)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    BigQuery     ‚îÇ (dataset.cat_facts - Particionada)
‚îÇ  (Gold Layer)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Analytics Teams ‚îÇ (Looker Studio, Data Studio, etc.)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Vantagens**:
- ‚úÖ Baixo custo operacional (pay-per-use)
- ‚úÖ Zero gerenciamento de infraestrutura
- ‚úÖ Escalabilidade autom√°tica
- ‚úÖ Implementa√ß√£o r√°pida e simples
- ‚úÖ Ideal para pipelines lineares

**Desvantagens**:
- ‚ùå Orquestra√ß√£o limitada para pipelines complexos
- ‚ùå Dif√≠cil gerenciar depend√™ncias entre m√∫ltiplas tarefas
- ‚ùå Menor visibilidade do pipeline completo
- ‚ùå Retry logic precisa ser implementado manualmente

---

#### üîπ **Arquitetura 2: Solu√ß√£o Robusta com Cloud Composer (Apache Airflow)**

**Ideal para**: Pipelines complexos, m√∫ltiplas depend√™ncias, alto volume, governan√ßa de dados

**Componentes**:
- **Cloud Composer (Airflow)**: Orquestra√ß√£o completa de workflows
- **Cloud Functions/Cloud Run**: Execu√ß√£o de tarefas espec√≠ficas
- **Cloud Storage**: Data Lake (camadas Bronze, Silver, Gold)
- **BigQuery**: Data Warehouse e processamento anal√≠tico
- **Cloud Dataproc** (opcional): Processamento Spark para grandes volumes
- **Cloud Logging/Monitoring**: Observabilidade e alertas
- **Secret Manager**: Gerenciamento seguro de credenciais

**Fluxo de dados (DAG Airflow)**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Cloud Composer (Airflow)                      ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  DAG: cat_facts_daily_pipeline                                   ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇ check_api    ‚îÇ (Verifica disponibilidade da API)              ‚îÇ
‚îÇ  ‚îÇ availability ‚îÇ                                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇextract_facts ‚îÇ (Cloud Function/Operator)                      ‚îÇ
‚îÇ  ‚îÇ   from_api   ‚îÇ ‚Üí Cloud Storage (Bronze)                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇvalidate_data ‚îÇ (Great Expectations/Python)                    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ                                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇ transform_   ‚îÇ (Dataproc/Dataflow/BigQuery)                   ‚îÇ
‚îÇ  ‚îÇ   and_clean  ‚îÇ ‚Üí Cloud Storage (Silver)                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇ  load_to_bq  ‚îÇ (BigQueryInsertJobOperator)                    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ ‚Üí BigQuery (Gold - Particionado)               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇ data_quality ‚îÇ (Testes de qualidade de dados)                 ‚îÇ
‚îÇ  ‚îÇ   checks     ‚îÇ                                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ         ‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ  ‚îÇ  send_slack  ‚îÇ (Notifica√ß√£o de sucesso/falha)                 ‚îÇ
‚îÇ  ‚îÇnotification  ‚îÇ                                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ    BigQuery      ‚îÇ
                  ‚îÇ  (dataset.       ‚îÇ
                  ‚îÇ   cat_facts)     ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ Analytics Teams  ‚îÇ
                  ‚îÇ & Applications   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Estrutura de Camadas (Medallion Architecture)**:
```
Cloud Storage:
‚îú‚îÄ‚îÄ bronze/          # Dados brutos da API (JSON)
‚îÇ   ‚îî‚îÄ‚îÄ cat_facts/
‚îÇ       ‚îî‚îÄ‚îÄ year=2026/
‚îÇ           ‚îî‚îÄ‚îÄ month=01/
‚îÇ               ‚îî‚îÄ‚îÄ day=26/
‚îÇ                   ‚îî‚îÄ‚îÄ cat_facts_20260126_*.json
‚îÇ
‚îú‚îÄ‚îÄ silver/          # Dados limpos e validados (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ cat_facts/
‚îÇ       ‚îî‚îÄ‚îÄ year=2026/
‚îÇ           ‚îî‚îÄ‚îÄ cat_facts_*.parquet
‚îÇ
‚îî‚îÄ‚îÄ gold/            # Dados agregados/otimizados (backup)
    ‚îî‚îÄ‚îÄ cat_facts/
        ‚îî‚îÄ‚îÄ cat_facts_analytics_*.parquet

BigQuery:
‚îî‚îÄ‚îÄ dataset_prod/
    ‚îú‚îÄ‚îÄ cat_facts (tabela principal - particionada)
    ‚îú‚îÄ‚îÄ cat_facts_staging
    ‚îî‚îÄ‚îÄ cat_facts_quality_metrics
```

**DAG Airflow - Principais Operators**:
```python
from airflow import DAG
from airflow.providers.google.cloud.operators.functions import CloudFunctionInvokeFunctionOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email': ['data-team@uolcatlovers.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'cat_facts_daily_pipeline',
    default_args=default_args,
    description='Pipeline di√°rio de Cat Facts',
    schedule_interval='0 2 * * *',  # 2 AM diariamente
    start_date=datetime(2026, 1, 1),
    catchup=False,
    tags=['cat-facts', 'daily', 'api'],
) as dag:
    
    # Tasks definidas no diagrama acima
    ...
```

**Vantagens**:
- ‚úÖ Orquestra√ß√£o visual e completa de workflows
- ‚úÖ Gerenciamento robusto de depend√™ncias e paralelismo
- ‚úÖ Retry autom√°tico e tratamento de falhas
- ‚úÖ Monitoramento detalhado de cada etapa
- ‚úÖ Versionamento de DAGs
- ‚úÖ Backfill e reprocessamento facilitado
- ‚úÖ Integra√ß√£o nativa com todo ecossistema GCP
- ‚úÖ SLAs e alertas configur√°veis

**Desvantagens**:
- ‚ùå Custo mais elevado (ambiente Composer sempre ativo)
- ‚ùå Curva de aprendizado do Airflow
- ‚ùå Maior complexidade inicial de setup
- ‚ùå Overhead para pipelines muito simples

---

#### üìä **Compara√ß√£o entre as Arquiteturas**

| Crit√©rio | Cloud Scheduler | Cloud Composer (Airflow) |
|----------|-----------------|--------------------------|
| **Complexidade** | Baixa | M√©dia-Alta |
| **Custo** | Muito baixo | M√©dio-Alto |
| **Escalabilidade** | Alta (serverless) | Muito Alta |
| **Orquestra√ß√£o** | Limitada | Completa |
| **Visibilidade** | B√°sica (logs) | Avan√ßada (UI Airflow) |
| **Manuten√ß√£o** | M√≠nima | Moderada |
| **Retry Logic** | Manual | Autom√°tico |
| **Depend√™ncias** | Dif√≠cil | Nativo |
| **Ideal para** | MVPs, pipelines simples | Produ√ß√£o enterprise |
| **Time to Market** | R√°pido (dias) | M√©dio (semanas) |

---

#### üéØ **Recomenda√ß√£o**

**Para a UOLCatLovers**:

- **Fase Inicial (MVP)**: Usar **Arquitetura 1 (Cloud Scheduler)** 
  - Baixo custo e r√°pida implementa√ß√£o
  - Suficiente para validar o produto

- **Crescimento**: Migrar para **Arquitetura 2 (Airflow)** quando:
  - Volume de dados > 1GB/dia
  - Necessidade de m√∫ltiplas fontes de dados
  - Pipelines com mais de 3 etapas interdependentes
  - Time de dados com 2+ engenheiros
  - Necessidade de governan√ßa e compliance

---

### 3. Esquema da Tabela BigQuery

**Objetivo**: Especificar o esquema da tabela de cat facts no BigQuery.

**Localiza√ß√£o**: `schema/cat_facts_bigquery_schema.json`

**Campos da tabela `cat_facts`**:

| Campo | Tipo | Modo | Descri√ß√£o |
|-------|------|------|-----------|
| id | STRING | REQUIRED | Identificador √∫nico do fato |
| text | STRING | REQUIRED | Texto do fato sobre gatos |
| type | STRING | NULLABLE | Tipo/categoria do fato |
| user_id | STRING | NULLABLE | ID do usu√°rio que criou o fato |
| upvotes | INTEGER | NULLABLE | N√∫mero de votos positivos |
| created_at | TIMESTAMP | REQUIRED | Data/hora de cria√ß√£o |
| updated_at | TIMESTAMP | REQUIRED | Data/hora da √∫ltima atualiza√ß√£o |
| deleted | BOOLEAN | NULLABLE | Indicador se o fato foi deletado |
| source | STRING | NULLABLE | Origem do fato |
| used | BOOLEAN | NULLABLE | Indicador se o fato foi usado |
| ingestion_date | DATE | REQUIRED | Data de ingest√£o no BigQuery |
| ingestion_timestamp | TIMESTAMP | REQUIRED | Timestamp de ingest√£o no BigQuery |

**Considera√ß√µes**:
- Particionamento por `ingestion_date` para otimiza√ß√£o de consultas e custo
- Clustering por `updated_at` para queries temporais
- Campos de auditoria (`ingestion_date`, `ingestion_timestamp`) para rastreabilidade

---

### 4. Consulta SQL - Fatos Atualizados em Agosto/2020

**Objetivo**: Extrair fatos atualizados em agosto de 2020 para estudo de caso do time de Analytics.

**Localiza√ß√£o**: `src/queries/august_2020_facts.sql`

**Query**:
```sql
SELECT 
    id,
    text,
    type,
    user_id,
    upvotes,
    created_at,
    updated_at,
    deleted,
    source,
    used
FROM 
    `project-id.dataset.cat_facts`
WHERE 
    DATE(updated_at) BETWEEN '2020-08-01' AND '2020-08-31'
    AND deleted IS NOT TRUE
ORDER BY 
    updated_at DESC;
```

---

### 5. Consulta SQL - Amostra Aleat√≥ria 10% para QA

**Objetivo**: Extrair 10% aleat√≥rio dos registros para ambiente de QA, exportando para CSV.

**Localiza√ß√£o**: `src/queries/random_sample_10pct.sql`

**Query**:
```sql
SELECT 
    text,
    created_at,
    updated_at
FROM 
    `project-id.dataset.cat_facts`
WHERE 
    RAND() < 0.1
ORDER BY 
    RAND();
```

**Exporta√ß√£o para CSV**:
```bash
bq extract \
  --destination_format CSV \
  --field_delimiter ',' \
  --print_header=true \
  'project-id:dataset.cat_facts_sample' \
  gs://bucket-name/cat_facts_qa_sample.csv
```

**Alternativa usando CLI do BigQuery**:
```bash
bq query \
  --use_legacy_sql=false \
  --format=csv \
  --max_rows=1000000 \
  'SELECT text, created_at, updated_at 
   FROM `project-id.dataset.cat_facts` 
   WHERE RAND() < 0.1 
   ORDER BY RAND()' > cat_facts_qa_sample.csv
```

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.8+**: Linguagem principal para scripts
- **Google Cloud Platform**: Infraestrutura em nuvem
  - BigQuery
  - Cloud Functions
  - Cloud Storage
  - Cloud Scheduler
  - Dataflow
- **SQL**: Consultas anal√≠ticas
- **Libraries Python**:
  - `requests`: Consumo da API
  - `pandas`: Manipula√ß√£o de dados
  - `google-cloud-bigquery`: Integra√ß√£o com BigQuery
  - `google-cloud-storage`: Integra√ß√£o com Cloud Storage

---

## üìä Dados da API Cat Facts

A API fornece os seguintes endpoints principais:

- `GET /facts`: Lista todos os fatos sobre gatos
- `GET /facts/random`: Retorna um fato aleat√≥rio
- `GET /facts/{id}`: Retorna um fato espec√≠fico

**Exemplo de resposta**:
```json
{
  "_id": "591f9894d369931519ce358f",
  "text": "A cat's hearing is better than a dog's.",
  "type": "cat",
  "user": {
    "_id": "587288bb2f814b9c57a9040f",
    "name": {
      "first": "Alex",
      "last": "Wohlbruck"
    }
  },
  "upvotes": 4,
  "userUpvoted": null,
  "createdAt": "2018-01-04T01:10:54.673Z",
  "updatedAt": "2020-08-23T20:20:01.611Z"
}
```

---

## üìù Requisitos

### Para execu√ß√£o local (Quest√£o 1):
```bash
pip install requests pandas python-dotenv
```

### Para integra√ß√£o com GCP:
```bash
pip install google-cloud-bigquery google-cloud-storage
```

**Arquivo `requirements.txt`**:
```
requests>=2.28.0
pandas>=1.5.0
google-cloud-bigquery>=3.0.0
google-cloud-storage>=2.0.0
python-dotenv>=0.20.0
```

---

## üîí Boas Pr√°ticas Implementadas

1. **Seguran√ßa**:
   - Credenciais armazenadas em vari√°veis de ambiente
   - Service accounts com permiss√µes m√≠nimas necess√°rias
   
2. **Qualidade de Dados**:
   - Valida√ß√£o de dados antes da ingest√£o
   - Deduplica√ß√£o baseada em ID
   - Tratamento de valores nulos
   
3. **Performance**:
   - Particionamento de tabelas BigQuery
   - Clustering para otimiza√ß√£o de queries
   - Batch processing para volumes grandes
   
4. **Monitoramento**:
   - Logging estruturado
   - Alertas para falhas de pipeline
   - M√©tricas de SLA

5. **Documenta√ß√£o**:
   - README detalhado
   - Coment√°rios no c√≥digo
   - Esquemas versionados

---

## üìà Evolu√ß√£o da Solu√ß√£o

### Fase 1: Solu√ß√£o Local (MVP)
- Script Python simples
- Armazenamento em CSV local
- Execu√ß√£o manual

### Fase 2: Solu√ß√£o em Nuvem (Escal√°vel)
- Arquitetura serverless na GCP
- Processamento automatizado
- Armazenamento escal√°vel
- Acesso via BigQuery

### Fase 3: Pr√≥ximos Passos
- [ ] Implementar CI/CD com Cloud Build
- [ ] Adicionar testes automatizados
- [ ] Criar dashboards no Power BI ou Looker
- [ ] Implementar Data Quality checks com Great Expectations
- [ ] Adicionar streaming com Pub/Sub para dados em tempo real

---

## üë• Time

**Engenharia de Dados - UOLCatLovers**

---

## üìÑ Licen√ßa

Este projeto foi desenvolvido como parte de um case t√©cnico para avalia√ß√£o.

---

## üîó Links √öteis

- [Cat Facts API Documentation](https://alexwohlbruck.github.io/cat-facts/docs/)
- [Google Cloud BigQuery Documentation](https://cloud.google.com/bigquery/docs)
- [Google Cloud Functions Documentation](https://cloud.google.com/functions/docs)
- [Python Requests Library](https://requests.readthedocs.io/)

---

**Desenvolvido com ‚ù§Ô∏è e üê± pela equipe UOLCatLovers**
